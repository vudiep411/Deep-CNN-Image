{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b82732-d06d-4190-9f93-302f84efc9e3",
   "metadata": {},
   "source": [
    "# Set up GPU\n",
    "### Start setting up gpu for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1defbe-308b-4512-b935-7cabdbff1b33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06796589-e6bf-4a28-8adf-3af749665831",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af735d21-78dc-40d5-9bfa-0348bcd4914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36dec3d-2292-4ba5-b1af-7f74dd506a3d",
   "metadata": {},
   "source": [
    "# Remove Dodgy Images (optional)\n",
    "### Import and start removing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f8dbfc-963f-43e4-ab5b-6dc3f3a73a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imghdr\n",
    "import time\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a801b8b0-6703-484d-8962-f5f08a0ecb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5570dcbb-e48b-4c4d-8574-91183bada207",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_exts = ['jpeg', 'jpg', 'bmp', 'png']\n",
    "start_time = time.time()\n",
    "for image_class in os.listdir(DATA_DIR):\n",
    "    for image in os.listdir(os.path.join(DATA_DIR, image_class)):\n",
    "        image_path = os.path.join(DATA_DIR, image_class, image)\n",
    "        try:\n",
    "            img = cv2.imread(image_path)\n",
    "            tip = imghdr.what(image_path)\n",
    "            if tip not in image_exts:\n",
    "                print(\"Invalid Image\")\n",
    "                os.remove(image_path)   # Remove an image\n",
    "        except Exception as e:\n",
    "            print(f\"Issue with image {image_path}\")\n",
    "end_time = time.time()   \n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733d125a-d922-4a86-862d-fc2317685d96",
   "metadata": {},
   "source": [
    "### Visualize an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6549f74-8e12-4afa-9242-f4bb6e1f0149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to visualize an image\n",
    "img = cv2.imread(os.path.join(DATA_DIR, 'male', '090544.jpg.jpg'))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d4bb56-a08a-4a00-a784-c81e3dc6a5f8",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "### Using keras utils library to load images\n",
    "Will be using `image_dataset_from_directory()` from keras to load images from the `./data` directory to **data** variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3fde19-1ec8-45d9-9c93-1bb684549121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90fa8d-4440-4baa-b01b-0717b19121b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fe75fe-c226-49d4-900b-4802d23b80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image classification by tensorflow\n",
    "data = tf.keras.utils.image_dataset_from_directory(DATA_DIR, image_size=(256, 256))\n",
    "data_iterator = data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d74417-e737-4308-ab69-96dbc2739cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_iterator.next()\n",
    "# Images\n",
    "# print(batch[0])\n",
    "# Labels\n",
    "print(batch[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911db324-2df3-4873-b80b-1b5b1c6baf5e",
   "metadata": {},
   "source": [
    "### Check the class of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093b8e69-308d-42af-9252-de06d44e78a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1 is male 2 is female\n",
    "fix, ax = plt.subplots(ncols=4, figsize=(20, 20))\n",
    "for idx, img in enumerate(batch[0][:4]):\n",
    "    ax[idx].imshow(img.astype(int))\n",
    "    ax[idx].title.set_text(batch[1][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57c1cd7-bf7a-4e30-a8cf-0b7960c821df",
   "metadata": {},
   "source": [
    "# Preprocess Data\n",
    "### Start Scaling data\n",
    "This step we scale the data to fit between 0-1 by dividing by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a61bdc-4f03-48fa-962d-caadd5fe5c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "data = data.map(lambda x, y: (x/255, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a951480-6c24-46df-a021-92c04bcfd21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_it = data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e9da61-e670-41bc-9c84-3ec293f5c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_it.next()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1cae86-d2b1-4076-9ac0-8fcfed22450b",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b137b7ba-9cdd-4df3-8cf7-5d7c07a59b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data) * .7) + 2 # 70%\n",
    "val_size = int(len(data) * .2)   # 20%\n",
    "test_size = int(len(data) * .1)  # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fc4e2b-1f18-4b0a-a309-94daa8dae5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size + val_size + test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96c4d9d-e945-489d-a795-5758d2d76933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for data\n",
    "train_data = data.take(train_size)\n",
    "val = data.skip(train_size).take(val_size)\n",
    "test = data.skip(train_size+val_size).take(test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347fb843-b0d0-48ac-8112-635577f56de9",
   "metadata": {},
   "source": [
    "# Deep Model\n",
    "### Initilize model\n",
    "Use Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb222e8-f6fb-4624-a221-18e806a60aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8888f654-b18d-4d51-96dd-8d54cba914a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Sequential model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1554b150-1c43-4962-9219-d8ce3288e229",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width = 256\n",
    "image_height = 256\n",
    "channels = 3\n",
    "num_classes = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f14d86-a348-472c-b93a-876a9a8ac17a",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be6209b-285e-4753-8bab-1dc57e1330bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model architecture\n",
    "model.add(Conv2D(16, (3,3), 1, activation='relu', input_shape=(image_width,image_height,channels))) # 128 x 128 by 3 channels\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(32, (3,3), 1, activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(64, (3,3), 1, activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fae7f9-7b49-4715-b363-4da27fe806cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=tf.losses.BinaryCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3db71e2-dd12-4df3-a631-93581c344688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1122801f-2cdb-4c7f-b038-c61935748f8d",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7dbd2a-014d-41e1-aea3-975a125fb4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log directory\n",
    "LOG_DIR = 'logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbbbef6-953a-4009-846c-ff8ad39151a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cdd12c-28dd-435a-9156-da4a51560c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train_data, epochs=15, validation_data=val, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1509af37-b9d1-43a6-b98b-9de0db01181a",
   "metadata": {},
   "source": [
    "### Plot Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4477815-1e6c-4e57-a4ac-46acd5df1d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(hist.history['loss'], color='teal', label='loss')\n",
    "plt.plot(hist.history['val_loss'], color='orange', label='val_loss')\n",
    "fig.suptitle('Loss', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5bd97c-bb00-4036-bac6-e0f5a693cc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "fig.suptitle('Accuracy', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c834c374-15b8-4a7a-b20e-ad910051b173",
   "metadata": {},
   "source": [
    "# Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa27fd3-ebab-4287-8017-f7546bf6afae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b02064-af29-4b58-8171-0fc0a96fab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = Precision()\n",
    "recall = Recall()\n",
    "accuracy = BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c313d6b5-e26f-49e2-acaa-981534bae05a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for batch in test.as_numpy_iterator():\n",
    "    X, y = batch\n",
    "    yhat = model.predict(X)\n",
    "    precision.update_state(y, yhat)\n",
    "    recall.update_state(y, yhat)\n",
    "    accuracy.update_state(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6012824e-9138-4db6-980e-c5f9866cb4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{precision.result()}, {recall.result()}, {accuracy.result()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4a8a6c-2048-4f73-bd64-f63685eda47d",
   "metadata": {},
   "source": [
    "# Custom Test\n",
    "Download an image an place it in the root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825dad08-2752-4fa8-90ba-7a69ab1934ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in an image\n",
    "test1_img_dir = 'testfm.jpg'\n",
    "test2_img_dir = 'testm.jpg'\n",
    "img = cv2.imread(test1_img_dir)\n",
    "\n",
    "# Resize to 256x256\n",
    "resize_img = tf.image.resize(img, (256, 256))\n",
    "plt.imshow(resize_img.numpy().astype(int))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2350fa8-6f1d-4d55-bc26-bffc3d6290b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(np.expand_dims(resize_img/255, 0))\n",
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23099b34-7c67-47ea-9431-3e0a03e4c00c",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fdf6ed-c237-4ee5-bb85-27102c8a0a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f4828b-7e86-495d-b5f6-2a5449e89221",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join('models','imageclassifier.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7f730420-61ef-4e20-8821-68c0068b4808",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model('./models/imageclassifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e4627436-8c57-429c-b29e-971430ef8d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.predict(np.expand_dims(resize_img/255, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cffc908-f757-4444-968d-88f215c8b581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn",
   "language": "python",
   "name": "cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
